{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How to load our images\n",
    "### 1.1 MNIST-like load_data function\n",
    "### 1.2 ImageDataGenerator\n",
    "### 1.3 Dataframes\n",
    "\n",
    "## 2. MNIST\n",
    "## 3. Data augmentation\n",
    "## 4. Pre-trained neural networks\n",
    "## 5. RGB or grayscale\n",
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying album covers to genres\n",
    "\n",
    "In the project we try to classify album covers to genres. The dataset has 5022 pictures, sepreated in five categories. The categories are Rock, Pop, Electronic, Jazz and HipHop. One of the challenges of classifying album covers is that there is no systematic which can be applied on covers. For example when we try to classify a picture as dog or cat, we humans can clearly se the difference. And the machine learning algorithm can try to detected shapes that are common for cats and dogs. But when humans look at an album cover, provided that they don't have any prior knowledge, it's classify an album cover. That first thought about the dataset leads to an early thesis that we probalby need an pre-trained neural network for solving this task appropriate.\n",
    "Our approach is to start with traditional machine learning methods, like using an convolutional neural network inspired by the architecture used for classifying handwritten didgets from MNIST. \n",
    "After that we have an baseline for comparing with other methods and possible improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some comfort functions\n",
    "\n",
    "The cells below will import all needed libraries and also test if the systme is set up correctly. We don't need to duplicate the imports. We just import once before we run other cells with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# keras\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.applications import VGG16\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "# sci kit learm\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import ImageChops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.2.0\n",
      "Keras Version: 2.3.0-tf\n",
      "\n",
      "Python 3.8.5 (default, Sep  5 2020, 10:50:12) \n",
      "[GCC 10.2.0]\n",
      "Pandas 1.0.5\n",
      "Scikit-Learn 0.23.1\n",
      "WARNING:tensorflow:From <ipython-input-2-86f9a4b4bc68>:7: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How to load images\n",
    "\n",
    "The dataset consists out of 5022 images seperated in the five categories Rock, Pop, Electronic, Jazz and HipHop.\n",
    "For each category exists one folder with the associated images. We use three different approaches to load the data.\n",
    "Our first approach is inspired by the MNIST dataset and we wrote our own `load_data` function. The second approach is the use of an `ImageDataGenerator` that needs a specific directory structure. And the last one is to use dataframes, which are csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MNIST-like load_data function\n",
    "\n",
    "In the MNIST example the data is loades via `load_data()` function, which returns two tuple with training and test data. This function is imitated by our `load_data()` function.\n",
    "The images\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define some useful constants\n",
    "DATA_PATH = './data/covers_original/'\n",
    "CATEGORIES = ['electronic', 'hiphop', 'jazz', 'pop', 'rock']\n",
    "IMG_SIZE = 150\n",
    "CATEGORIES_SIZE = 5\n",
    "TRAIN_PERC = 0.8\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "# define a function that creates the dataset\n",
    "def load_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    # training data\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATA_PATH, category) # '../data/covers_original/<category>'\n",
    "        cn = CATEGORIES.index(category) # get index of class name (e.g. 'electronic' => 0, 'rock' => 4)\n",
    "        \n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img = load_img(os.path.join(path, img), target_size=(IMG_SIZE, IMG_SIZE))\n",
    "                img_as_array = img_to_array(img)\n",
    "                X.append(img_as_array)\n",
    "                y.append(cn)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    X = np.array(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_PERC)\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "# reshape and normalize the data\n",
    "X_train = X_train.reshape((4000, IMG_SIZE, IMG_SIZE, 3))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.reshape((1000, IMG_SIZE, IMG_SIZE, 3))\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# one-hot encode the class labels (0-9)\n",
    "y_train = np_utils.to_categorical(y_train, CATEGORIES_SIZE)\n",
    "y_test = np_utils.to_categorical(y_test, CATEGORIES_SIZE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
